{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "import argparse\n",
    "from utils.utilities import int16_to_float32, move_data_to_device, set_labels\n",
    "\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "from scipy.stats import norm\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from captum.attr import Deconvolution, GuidedBackprop, NeuronDeconvolution, NeuronGuidedBackprop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust according to your experiment\n",
    "ref_fold = \"8\"\n",
    "run_id = \"\"\n",
    "tracking_server = \"\"\n",
    "workspace_file = \"\"\n",
    "dataset_dir = \"\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "mlflow.set_tracking_uri(f\"{tracking_server}:5000\")\n",
    "logged_model = mlflow.pytorch.load_model(f\"runs:/{run_id}/models\")\n",
    "logged_model = logged_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(workspace_file, 'r') as h5_file:\n",
    "    folds = h5_file['fold'][:].astype(np.float32)\n",
    "\n",
    "    indexes = np.where(folds == int(ref_fold))[0]\n",
    "    inp_data = int16_to_float32(h5_file[\"waveform\"][indexes])\n",
    "    inp_data = move_data_to_device(inp_data, device)\n",
    "    labels = h5_file[\"target\"][indexes]\n",
    "    labels = move_data_to_device(labels, device)\n",
    "\n",
    "_, lb_to_idx = set_labels(dataset_dir)\n",
    "idx_to_label = {idx: label for label, idx in lb_to_idx.items()}\n",
    "target = [idx for label, idx in lb_to_idx.items() if label.startswith(\"albilora\")]\n",
    "# allows gradient for the method\n",
    "inp_data.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar method to plot the attributions\n",
    "def plot_frame_attributions(attributions, title=\"Average Frames importance using Deconvolution\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(np.arange(len(attributions[0])), np.mean(attributions.cpu().detach().numpy(), axis=0))\n",
    "    plt.xlabel(\"Frames\")\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_attribution(model, layer, layer_name, inp_data, neuron, algorithm=\"deconv\", verbose=False):\n",
    "    channels = neuron[0]\n",
    "    time_steps = neuron[1]\n",
    "    mel_bins = neuron[2]\n",
    "    \n",
    "    tot = 0\n",
    "    iterations = channels * time_steps * mel_bins\n",
    "    \n",
    "    if algorithm == \"deconv\":\n",
    "        layer_deconv = NeuronDeconvolution(model, layer)\n",
    "    elif algorithm == \"guided\":\n",
    "        layer_deconv = NeuronGuidedBackprop(model, layer)\n",
    "    else:\n",
    "        raise ValueError(f\"Incorrect algorithm {algorithm}. Expected 'deconv' or 'guided'\")\n",
    "        \n",
    "    out_dict = {\"layer\": [], \"channel\": [], \"time_steps\": [], \"mel_bins\": [], \"frameAvgAttr\": []}\n",
    "    \n",
    "    for channel in range(channels):\n",
    "        for time_step in range(time_steps):\n",
    "            for mel_bin in range(mel_bins):\n",
    "                conv1_neuron_attr = layer_deconv.attribute(inp_data, (channel, time_step, mel_bin))\n",
    "                out_dict[\"layer\"].append(layer_name)\n",
    "                out_dict[\"channel\"].append(channel)\n",
    "                out_dict[\"time_steps\"].append(time_step)\n",
    "                out_dict[\"mel_bins\"].append(mel_bin)\n",
    "                out_dict[\"frameAvgAttr\"].append(np.mean(np.mean(conv1_neuron_attr.cpu().detach().numpy(), axis=0)))\n",
    "                if verbose:\n",
    "                    tot += 1\n",
    "                    print(f\"Progress: {tot}/{iterations}-----------{100*tot/iterations:.2f}%\", end=\"\\r\")\n",
    "                                                \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ZCR to possible percussive audios\n",
    "# https://github.com/tyiannak/pyAudioAnalysis\n",
    "def zero_crossing_rate(frame):\n",
    "    count = len(frame)\n",
    "    count_zero = np.sum(np.abs(np.diff(np.sign(frame)))) / 2\n",
    "    return np.float64(count_zero) / np.float64(count - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(135)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zeiler and Fergus (2014)- Deconvolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deconv = Deconvolution(logged_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular atribuições para o canto e cada sílaba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_attribution = deconv.attribute(inp_data, target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syl1_attribution = deconv.attribute(inp_data, target[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syl2_attribution = deconv.attribute(inp_data, target[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frame_attributions(ca_attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frame_attributions(syl1_attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frame_attributions(syl2_attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The detected class(es) for the first experiment with 100 iterations\n",
    "others_attribution = deconv.attribute(inp_data, lb_to_idx[\"others\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frame_attributions(others_attribution)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was a significant impact in attribution for the middle of the audio. Maybe the way I've selected the audios has influency for greather attributions between 0.25 seconds and 0.625 seconds (5000/24000 and 15000/24000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the behavior above for background sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savanna = [idx for label, idx in lb_to_idx.items() if label.startswith(\"SAV\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sav_dry_attributions = deconv.attribute(inp_data, savanna[0])\n",
    "plot_frame_attributions(sav_dry_attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sav_wet_attributions = deconv.attribute(inp_data, savanna[1])\n",
    "plot_frame_attributions(sav_wet_attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = [idx for label, idx in lb_to_idx.items() if label.startswith(\"FOR\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_dry_attributions = deconv.attribute(inp_data, forest[0])\n",
    "plot_frame_attributions(for_dry_attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_wet_attributions = deconv.attribute(inp_data, forest[1])\n",
    "plot_frame_attributions(for_wet_attributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this behavior the same for individual layers?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Convolutional Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_deconv_conv1 = NeuronDeconvolution(logged_model, logged_model.base.conv_block1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neuron's indices: (0..63, 0..37, 0..31) - (channels, time_steps or num_frames, mel_bins), i.e, Neuron's output dimension\n",
    "- channels always doubling\n",
    "- num_frames = 1+ceil(len_y / hop_length) if center is True\n",
    "- else 1 + ceil(len_y - n_fft) / hop_length where len_y is the length of the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_ca_attributions = neuron_deconv_conv1.attribute(inp_data, (0, 37, 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frame_attributions(neuron_ca_attributions, title=\"Average Frames importance for a Neuron on 1st Conv Block\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again for the same block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_ca_attributions_2 = neuron_deconv_conv1.attribute(inp_data, (33, 15, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frame_attributions(neuron_ca_attributions_2, title=\"Average Frames importance for a Neuron on 1st Conv Block\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer attribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's too much time run the algorithm for all 77824 neurons. I will define a bootrasp distribution from some neurons to speed up processing and minimize biases on analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentative to centralize as cubic root of 360 as 7.11\n",
    "samples = 360 # Approximately\n",
    "channels = rng.integers(64)\n",
    "time_steps = rng.integers(np.floor(samples/channels))\n",
    "mel_bins = int(np.ceil(samples / (channels * time_steps)))\n",
    "print(channels * time_steps * mel_bins, channels, time_steps, mel_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "frames_attr_layer_1_deconv_meta = layer_attribution(\n",
    "    logged_model,\n",
    "    logged_model.base.conv_block1,\n",
    "    \"Conv 1 block\",\n",
    "    inp_data,\n",
    "    (channels, time_steps, mel_bins), verbose=True\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(frames_attr_layer_1_deconv_meta)\n",
    "\n",
    "attr_mean_bootstrap = []\n",
    "for i in range(10000):\n",
    "    attr_mean_bootstrap.append(df.sample(frac=1, replace=True)[\"frameAvgAttr\"])\n",
    "\n",
    "std_error = np.std(attr_mean_bootstrap, ddof=1)\n",
    "pop_std_error = std_error * np.sqrt(samples)\n",
    "print(std_error, pop_std_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(attr_mean_bootstrap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_estimate = np.mean(attr_mean_bootstrap)\n",
    "lower = norm.ppf(0.05, loc=point_estimate, scale=std_error)\n",
    "upper = norm.ppf(0.95, loc=point_estimate, scale=std_error)\n",
    "print(lower, upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "frames_attr_layer_2_deconv_meta = layer_attribution(\n",
    "    logged_model,\n",
    "    logged_model.base.conv_block2,\n",
    "    \"Conv 2 block\",\n",
    "    inp_data,\n",
    "    (channels, time_steps, mel_bins), verbose=True\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(frames_attr_layer_2_deconv_meta)\n",
    "\n",
    "attr_mean_bootstrap = []\n",
    "for i in range(10000):\n",
    "    attr_mean_bootstrap.append(df.sample(frac=1, replace=True)[\"frameAvgAttr\"])\n",
    "\n",
    "std_error = np.std(attr_mean_bootstrap, ddof=1)\n",
    "pop_std_error = std_error * np.sqrt(samples)\n",
    "print(std_error, pop_std_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(attr_mean_bootstrap));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_estimate = np.mean(attr_mean_bootstrap)\n",
    "lower = norm.ppf(0.05, loc=point_estimate, scale=std_error)\n",
    "upper = norm.ppf(0.95, loc=point_estimate, scale=std_error)\n",
    "print(lower, upper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5th layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 2\n",
    "mel_bins = 2\n",
    "channels = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "frames_attr_layer_5_deconv_meta = layer_attribution(\n",
    "    logged_model,\n",
    "    logged_model.base.conv_block5,\n",
    "    \"Conv 5 block\",\n",
    "    inp_data,\n",
    "    (channels, time_steps, mel_bins),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(frames_attr_layer_5_deconv_meta)\n",
    "\n",
    "attr_mean_bootstrap = []\n",
    "for i in range(10000):\n",
    "    attr_mean_bootstrap.append(df.sample(frac=1, replace=True)[\"frameAvgAttr\"])\n",
    "\n",
    "std_error = np.std(attr_mean_bootstrap, ddof=1)\n",
    "pop_std_error = std_error * np.sqrt(samples)\n",
    "print(std_error, pop_std_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(attr_mean_bootstrap));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_estimate = np.mean(attr_mean_bootstrap)\n",
    "lower = norm.ppf(0.05, loc=point_estimate, scale=std_error)\n",
    "upper = norm.ppf(0.95, loc=point_estimate, scale=std_error)\n",
    "print(lower, upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Springebenberg et al. 2015- Guided Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_backprop = GuidedBackprop(logged_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call and syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_attribution = guided_backprop.attribute(inp_data, target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syl1_attribution = guided_backprop.attribute(inp_data, target[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syl2_attribution = guided_backprop.attribute(inp_data, target[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frame_attributions(ca_attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frame_attributions(syl1_attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frame_attributions(syl2_attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The detected class(es) for the first experiment with 100 iterations\n",
    "others_attribution = deconv.attribute(inp_data, lb_to_idx[\"others\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frame_attributions(others_attribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savanna = [idx for label, idx in lb_to_idx.items() if label.startswith(\"SAV\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sav_dry_attributions = deconv.attribute(inp_data, savanna[0])\n",
    "plot_frame_attributions(sav_dry_attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sav_wet_attributions = deconv.attribute(inp_data, savanna[1])\n",
    "plot_frame_attributions(sav_wet_attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = [idx for label, idx in lb_to_idx.items() if label.startswith(\"FOR\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_dry_attributions = deconv.attribute(inp_data, forest[0])\n",
    "plot_frame_attributions(for_dry_attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_wet_attributions = deconv.attribute(inp_data, forest[1])\n",
    "plot_frame_attributions(for_wet_attributions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "frames_attr_layer_1_guided_meta = layer_attribution(\n",
    "    logged_model,\n",
    "    logged_model.base.conv_block1,\n",
    "    \"Conv 1 block\",\n",
    "    inp_data,\n",
    "    (channels, time_steps, mel_bins),\n",
    "    \"guided\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(frames_attr_layer_1_guided_meta)\n",
    "\n",
    "attr_mean_bootstrap = []\n",
    "for i in range(10000):\n",
    "    attr_mean_bootstrap.append(df.sample(frac=1, replace=True)[\"frameAvgAttr\"])\n",
    "\n",
    "std_error = np.std(attr_mean_bootstrap, ddof=1)\n",
    "pop_std_error = std_error * np.sqrt(samples)\n",
    "print(std_error, pop_std_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(attr_mean_bootstrap));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_estimate = np.mean(attr_mean_bootstrap)\n",
    "lower = norm.ppf(0.05, loc=point_estimate, scale=std_error)\n",
    "upper = norm.ppf(0.95, loc=point_estimate, scale=std_error)\n",
    "print(lower, upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "frames_attr_layer_2_guided_meta = layer_attribution(\n",
    "    logged_model,\n",
    "    logged_model.base.conv_block2,\n",
    "    \"Conv 2 block\",\n",
    "    inp_data,\n",
    "    (channels, time_steps, mel_bins),\n",
    "    \"guided\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(frames_attr_layer_2_guided_meta)\n",
    "\n",
    "attr_mean_bootstrap = []\n",
    "for i in range(10000):\n",
    "    attr_mean_bootstrap.append(df.sample(frac=1, replace=True)[\"frameAvgAttr\"])\n",
    "\n",
    "std_error = np.std(attr_mean_bootstrap, ddof=1)\n",
    "pop_std_error = std_error * np.sqrt(samples)\n",
    "print(std_error, pop_std_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(attr_mean_bootstrap));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_estimate = np.mean(attr_mean_bootstrap)\n",
    "lower = norm.ppf(0.05, loc=point_estimate, scale=std_error)\n",
    "upper = norm.ppf(0.95, loc=point_estimate, scale=std_error)\n",
    "print(lower, upper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5th layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 2\n",
    "mel_bins = 2\n",
    "channels = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "frames_attr_layer_5_guided_meta = layer_attribution(\n",
    "    logged_model,\n",
    "    logged_model.base.conv_block5,\n",
    "    \"Conv 5 block\",\n",
    "    inp_data,\n",
    "    (channels, time_steps, mel_bins),\n",
    "    \"guided\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(frames_attr_layer_5_guided_meta)\n",
    "\n",
    "attr_mean_bootstrap = []\n",
    "for i in range(10000):\n",
    "    attr_mean_bootstrap.append(df.sample(frac=1, replace=True)[\"frameAvgAttr\"])\n",
    "\n",
    "std_error = np.std(attr_mean_bootstrap, ddof=1)\n",
    "pop_std_error = std_error * np.sqrt(samples)\n",
    "print(std_error, pop_std_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(attr_mean_bootstrap));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_estimate = np.mean(attr_mean_bootstrap)\n",
    "lower = norm.ppf(0.05, loc=point_estimate, scale=std_error)\n",
    "upper = norm.ppf(0.95, loc=point_estimate, scale=std_error)\n",
    "print(lower, upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over each audio considering the parameters for spectrogram generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "\n",
    "client = mlflow.MlflowClient()\n",
    "run = client.get_run(run_id)\n",
    "run_data = run.data\n",
    "tags = run_data.tags\n",
    "\n",
    "window_size = int(tags[\"window_size\"])\n",
    "hop_size = int(tags[\"hop_size\"])\n",
    "cur_window = 0\n",
    "num_audios = len(inp_data)\n",
    "\n",
    "zcr_audios = {\"avgZcr\": [], \"label\": [], \"frame\": []}\n",
    "\n",
    "num_frame = 1\n",
    "while cur_window + window_size - 1 < inp_data.shape[1]:\n",
    "\n",
    "    for i in range(num_audios):\n",
    "        frame = inp_data[i][cur_window:cur_window+window_size]\n",
    "\n",
    "        frame_zcr = zero_crossing_rate(frame.cpu().detach().numpy())\n",
    "        zcr_audios[\"avgZcr\"].append(np.mean(frame_zcr))\n",
    "        zcr_audios[\"label\"].append(idx_to_label[np.argmax(labels.cpu().detach().numpy())])\n",
    "        zcr_audios[\"frame\"].append(num_frame)\n",
    "\n",
    "    num_frame += 1\n",
    "    cur_window += hop_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcr_audios = pd.DataFrame(zcr_audios)\n",
    "zcr_audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "sns.histplot(x=\"avgZcr\", data=zcr_audios, element=\"step\", fill=False, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_18_frames_zcr_audios = zcr_audios[zcr_audios[\"frame\"] <= 18]\n",
    "frames_18_36_zcr_audios = zcr_audios[(zcr_audios[\"frame\"] > 18) & (zcr_audios[\"frame\"] <= 36)]\n",
    "frames_36_54_zcr_audios = zcr_audios[(zcr_audios[\"frame\"] > 36) & (zcr_audios[\"frame\"] <= 54)]\n",
    "frames_54_above_zcr_audios = zcr_audios[zcr_audios[\"frame\"] > 54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "sns.histplot(x=\"avgZcr\", data=first_18_frames_zcr_audios, hue=\"frame\", element=\"step\", fill=False, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "sns.histplot(x=\"avgZcr\", data=frames_18_36_zcr_audios, hue=\"frame\", element=\"step\", fill=False, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "sns.histplot(x=\"avgZcr\", data=frames_36_54_zcr_audios, hue=\"frame\", element=\"step\", fill=False, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "sns.histplot(x=\"avgZcr\", data=frames_54_above_zcr_audios, hue=\"frame\", element=\"step\", fill=False, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check 10 random spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(18, 35), nrows=10)\n",
    "sr = int(tags[\"sample_rate\"])\n",
    "\n",
    "for i in range(10):\n",
    "    audio = rng.integers(num_audios)\n",
    "\n",
    "    librosa.display.specshow(np.abs(\n",
    "        librosa.stft(inp_data[audio].cpu().detach().numpy(),\n",
    "            n_fft=window_size, win_length=window_size, hop_length=hop_size, center=True)\n",
    "        ),\n",
    "        sr=sr, x_axis=\"time\", y_axis=\"linear\", hop_length=hop_size,\n",
    "        fmin=int(tags[\"fmin\"]), fmax=int(tags[\"fmax\"]), ax=axes[i]\n",
    "    )\n",
    "    axes[i].set_title(f\"{idx_to_label[np.argmax(labels[audio].cpu().detach().numpy())]} spectrogram\",\n",
    "                    {'fontsize': 11}, loc='left')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most activities are between 0.25 seconds and 0.5 seconds\n",
    "- Model gave more attention to the middle of the audio\n",
    "- Low level convolutional blocks had less attribution, maybe due to more specific feature extraction from AudioSet dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Global gradient algorithms\n",
    "- [ ] Pertubation algorithm\n",
    "\n",
    "\n",
    "- Approaches based on local gradient analysis\n",
    "Selected period approach:\n",
    "- [ ] Change duration for load each audio/hop_size\n",
    "\n",
    "Resolution period approach:\n",
    "- [ ] Change parameters for spectrogram\n",
    "\n",
    "Model approach\n",
    "- [ ] Train PANNs from scratch with our data (I believe it will decrease performance)\n",
    "- [ ] Fine-tune PANNs with more iterations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
