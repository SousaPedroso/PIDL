{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683936314537
        }
      },
      "outputs": [],
      "source": [
        "from interpretability_utilities import plot_frame_attributions, plot_audio_attributions\n",
        "from interpretability_utilities import load_workspace_file, zero_crossing_rate\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "from scipy.stats import norm\n",
        "\n",
        "import mlflow\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from captum.attr import Deconvolution, GuidedBackprop, NeuronDeconvolution, NeuronGuidedBackprop"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Settings and Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683936328141
        }
      },
      "outputs": [],
      "source": [
        "# Adjust according to your experiment\n",
        "ref_fold = \"8\"\n",
        "run_id = \"\"\n",
        "tracking_server = \"\"\n",
        "workspace_file = \"\"\n",
        "dataset_dir = \"\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "mlflow.set_tracking_uri(f\"{tracking_server}:5000\")\n",
        "logged_model = mlflow.pytorch.load_model(f\"runs:/{run_id}/models\")\n",
        "logged_model = logged_model.eval()\n",
        "\n",
        "\n",
        "client = mlflow.MlflowClient()\n",
        "run = client.get_run(run_id)\n",
        "run_data = run.data\n",
        "tags = run_data.tags\n",
        "\n",
        "# Feature visualization\n",
        "sr = int(tags[\"sample_rate\"])\n",
        "window_size = int(tags[\"window_size\"])\n",
        "hop_size = int(tags[\"hop_size\"])\n",
        "cur_window = 0\n",
        "\n",
        "zcr_audios = {\"avgZcr\": [], \"label\": [], \"Período (ms)\": []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683936330906
        }
      },
      "outputs": [],
      "source": [
        "inp_data, indexes, labels, _, lb_to_idx, _ = load_workspace_file(workspace_file, ref_fold,\n",
        "                                dataset_dir, device)\n",
        "\n",
        "idx_to_label = {idx: label for label, idx in lb_to_idx.items()}\n",
        "target = [idx for label, idx in lb_to_idx.items() if label.startswith(\"albilora\")]\n",
        "\n",
        "# check inputs attribution\n",
        "str_labels = [idx_to_label[int(i.cpu().detach().numpy())] for i in np.argmax(labels, axis=1)]\n",
        "\n",
        "inp_data.requires_grad_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683936333978
        }
      },
      "outputs": [],
      "source": [
        "def layer_attribution(model, layer, layer_name, inp_data, neuron, algorithm=\"deconv\", verbose=False):\n",
        "    channels = neuron[0]\n",
        "    time_steps = neuron[1]\n",
        "    mel_bins = neuron[2]\n",
        "    \n",
        "    tot = 0\n",
        "    iterations = channels * time_steps * mel_bins\n",
        "    \n",
        "    if algorithm == \"deconv\":\n",
        "        layer_deconv = NeuronDeconvolution(model, layer)\n",
        "    elif algorithm == \"guided\":\n",
        "        layer_deconv = NeuronGuidedBackprop(model, layer)\n",
        "    else:\n",
        "        raise ValueError(f\"Incorrect algorithm {algorithm}. Expected 'deconv' or 'guided'\")\n",
        "        \n",
        "    out_dict = {\"input\": [], \"layer\": [], \"channel\": [], \"time_steps\": [], \"mel_bins\": [], \"layer_avg_attr\": []}\n",
        "    data_size = inp_data.size()[0]\n",
        "    for channel in range(channels):\n",
        "        for time_step in range(time_steps):\n",
        "            for mel_bin in range(mel_bins):\n",
        "                conv1_neuron_attr = layer_deconv.attribute(inp_data, (channel, time_step, mel_bin))\n",
        "                out_dict[\"layer_avg_attr\"].extend(torch.mean(conv1_neuron_attr, dim=1).cpu().detach().numpy().tolist())\n",
        "                \n",
        "                if device == 'cuda':\n",
        "                    del conv1_neuron_attr\n",
        "                    torch.cuda.empty_cache()\n",
        "                    \n",
        "                out_dict[\"input\"].extend([i for i in range(data_size)])\n",
        "                out_dict[\"layer\"].extend([layer_name]*data_size)\n",
        "                out_dict[\"channel\"].extend([channel]*data_size)\n",
        "                out_dict[\"time_steps\"].extend([time_step]*data_size)\n",
        "                out_dict[\"mel_bins\"].extend([mel_bin]*data_size)\n",
        "                if verbose:\n",
        "                    tot += 1\n",
        "                    print(f\"Progress: {tot}/{iterations}-----------{100*tot/iterations:.2f}%\", end=\"\\r\")\n",
        "                                                \n",
        "    return out_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683936334813
        }
      },
      "outputs": [],
      "source": [
        "# Compute ZCR to possible percussive audios\n",
        "# https://github.com/tyiannak/pyAudioAnalysis\n",
        "def zero_crossing_rate(frame):\n",
        "    count = len(frame)\n",
        "    count_zero = np.sum(np.abs(np.diff(np.sign(frame)))) / 2\n",
        "    return np.float64(count_zero) / np.float64(count - 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683936335365
        }
      },
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(135)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683936341505
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Tentative to centralize as cubic root of 360 as 7.11\n",
        "samples = 360 # Approximately\n",
        "channels = rng.integers(64)\n",
        "time_steps = rng.integers(np.floor(samples/channels))\n",
        "mel_bins = int(np.floor(samples / (channels * time_steps)))\n",
        "print(channels * time_steps * mel_bins, channels, time_steps, mel_bins)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zeiler and Fergus (2014)- Deconvolution"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683912251621
        }
      },
      "outputs": [],
      "source": [
        "deconv = Deconvolution(logged_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "General attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683912262593
        }
      },
      "outputs": [],
      "source": [
        "gen_attribution = deconv.attribute(inp_data, target=np.argmax(labels, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683912297252
        }
      },
      "outputs": [],
      "source": [
        "plot_frame_attributions(gen_attribution, title=\"Atribuição média do modelo para cada frame do conjunto de validação usando Deconvolution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_audio_attributions(gen_attribution, \"Atribuição média do modelo para cada áudio do conjunto de validação usando Deconvolution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683912333334
        }
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.bar(np.arange(gen_attribution.size()[1]), np.mean(gen_attribution.cpu().detach().numpy(), axis=0))\n",
        "\n",
        "ax.set_xlabel(\"Tempo [s]\")\n",
        "ax.set_ylabel(\"Atribuição\")\n",
        "\n",
        "ax.set_xticks(np.arange(0, gen_attribution.size()[1], int(sr/10)))\n",
        "ax.set_xticklabels(np.arange(0, 0.75, 0.10, dtype=np.float32))\n",
        "\n",
        "ax.set_title(\"Atribuição média do modelo para cada frame do conjunto de validação usando Deconvolution\")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp_df = pd.DataFrame({\"Espécies\": str_labels,\n",
        "    \"Atribuição\": np.mean(gen_attribution.cpu().detach().numpy(), axis=1)})\n",
        "\n",
        "sns.stripplot(x=\"Atribuição\", y=\"Espécies\", data=temp_df, palette=\"deep\", hue=\"Espécies\", legend=False)\n",
        "del temp_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Neuron attribution"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Neuron's indices: (0..63, 0..37, 0..31) - (channels, time_steps or num_frames, mel_bins), i.e, Neuron's output dimension\n",
        "- channels always doubling\n",
        "- num_frames = 1+ceil(len_y / hop_length) if center is True\n",
        "- else 1 + ceil(len_y - n_fft) / hop_length where len_y is the length of the audio"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First Convolutional Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683912333998
        }
      },
      "outputs": [],
      "source": [
        "neuron_deconv_conv1 = NeuronDeconvolution(logged_model, logged_model.base.conv_block1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683912341819
        }
      },
      "outputs": [],
      "source": [
        "neuron_ca_attributions = neuron_deconv_conv1.attribute(inp_data, (0, 37, 31))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683912377848
        }
      },
      "outputs": [],
      "source": [
        "plot_frame_attributions(neuron_ca_attributions, title=\"Average Frames importance for a Neuron on 1st Conv Block\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again for the same block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683912386312
        }
      },
      "outputs": [],
      "source": [
        "neuron_ca_attributions_2 = neuron_deconv_conv1.attribute(inp_data, (33, 15, 12))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683912424633
        }
      },
      "outputs": [],
      "source": [
        "plot_frame_attributions(neuron_ca_attributions_2, title=\"Average Frames importance for a Neuron on 1st Conv Block\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Layer attribution"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It takes too much time running the algorithm for all 77824 neurons. I will define a bootrasp distribution from some neurons to speed up processing and minimize biases on analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### First block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "block_attr_block_1_deconv_meta = layer_attribution(\n",
        "    logged_model,\n",
        "    logged_model.base.conv_block1,\n",
        "    \"Conv 1 block\",\n",
        "    inp_data,\n",
        "    (channels, time_steps, mel_bins), verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683915620269
        }
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(block_attr_block_1_deconv_meta)\n",
        "df.to_csv(\"data/block_attr_block_1_deconv_meta.csv\", index=None)\n",
        "uniques = df[\"input\"].unique()\n",
        "\n",
        "attr_mean_bootstrap = {\"input\": [], \"attribution\": []}\n",
        "for i in range(10000):\n",
        "    attr_mean_bootstrap[\"attribution\"].extend(rng.choice(df[\"layer_avg_attr\"].values, size=len(uniques)))\n",
        "    \n",
        "\n",
        "for i in range(10000):\n",
        "    attr_mean_bootstrap[\"input\"].extend([j for j in range(len(uniques))])\n",
        "\n",
        "attr_mean_bootstrap = pd.DataFrame(attr_mean_bootstrap)\n",
        "std_error = np.std(attr_mean_bootstrap[\"attribution\"], ddof=1)\n",
        "pop_std_error = std_error * np.sqrt(len(df))\n",
        "print(std_error, pop_std_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683915621915
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "input_attr = attr_mean_bootstrap.groupby(\"input\")[\"attribution\"].mean()\n",
        "plt.plot(np.arange(len(uniques)), input_attr);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "90% confidence interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683915662752
        }
      },
      "outputs": [],
      "source": [
        "point_estimate = np.mean(attr_mean_bootstrap[\"attribution\"])\n",
        "lower = norm.ppf(0.05, loc=point_estimate, scale=std_error)\n",
        "upper = norm.ppf(0.95, loc=point_estimate, scale=std_error)\n",
        "print(lower, upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683916271731
        }
      },
      "outputs": [],
      "source": [
        "attr_mean_bootstrap.to_csv(\"data/block_bootstrap_attr_block_1_deconv_meta.csv\", index=None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Second block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "block_attr_block_2_deconv_meta = layer_attribution(\n",
        "    logged_model,\n",
        "    logged_model.base.conv_block2,\n",
        "    \"Conv 2 block\",\n",
        "    inp_data,\n",
        "    (channels, time_steps, mel_bins), verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683919935758
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(block_attr_block_2_deconv_meta)\n",
        "df.to_csv(\"data/block_attr_block_2_deconv_meta.csv\", index=None)\n",
        "uniques = df[\"input\"].unique()\n",
        "\n",
        "attr_mean_bootstrap = {\"input\": [], \"attribution\": []}\n",
        "for i in range(10000):\n",
        "    attr_mean_bootstrap[\"attribution\"].extend(rng.choice(df[\"layer_avg_attr\"].values, size=len(uniques)))\n",
        "    \n",
        "\n",
        "for i in range(10000):\n",
        "    attr_mean_bootstrap[\"input\"].extend([j for j in range(len(uniques))])\n",
        "\n",
        "attr_mean_bootstrap = pd.DataFrame(attr_mean_bootstrap)\n",
        "std_error = np.std(attr_mean_bootstrap[\"attribution\"], ddof=1)\n",
        "pop_std_error = std_error * np.sqrt(len(df))\n",
        "print(std_error, pop_std_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683919936712
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "input_attr = attr_mean_bootstrap.groupby(\"input\")[\"attribution\"].mean()\n",
        "plt.plot(np.arange(len(uniques)), input_attr);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "90% confidence interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683919937560
        }
      },
      "outputs": [],
      "source": [
        "point_estimate = np.mean(attr_mean_bootstrap[\"attribution\"])\n",
        "lower = norm.ppf(0.05, loc=point_estimate, scale=std_error)\n",
        "upper = norm.ppf(0.95, loc=point_estimate, scale=std_error)\n",
        "print(lower, upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683919938241
        }
      },
      "outputs": [],
      "source": [
        "attr_mean_bootstrap.to_csv(\"data/block_bootstrap_attr_block_2_deconv_meta.csv\", index=None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5th block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683919938903
        }
      },
      "outputs": [],
      "source": [
        "time_steps_5th_layer = 2\n",
        "mel_bins_5th_layer = 2\n",
        "channels_5th_layer = 90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "block_attr_block_5_deconv_meta = layer_attribution(\n",
        "    logged_model,\n",
        "    logged_model.base.conv_block5,\n",
        "    \"Conv 5 block\",\n",
        "    inp_data,\n",
        "    (channels_5th_layer, time_steps_5th_layer, mel_bins_5th_layer),\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683924350133
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(block_attr_block_5_deconv_meta)\n",
        "df.to_csv(\"data/block_attr_block_5_deconv_meta.csv\", index=None)\n",
        "uniques = df[\"input\"].unique()\n",
        "\n",
        "attr_mean_bootstrap = {\"input\": [], \"attribution\": []}\n",
        "for i in range(10000):\n",
        "    attr_mean_bootstrap[\"attribution\"].extend(rng.choice(df[\"layer_avg_attr\"].values, size=len(uniques)))\n",
        "    \n",
        "\n",
        "for i in range(10000):\n",
        "    attr_mean_bootstrap[\"input\"].extend([j for j in range(len(uniques))])\n",
        "\n",
        "attr_mean_bootstrap = pd.DataFrame(attr_mean_bootstrap)\n",
        "std_error = np.std(attr_mean_bootstrap[\"attribution\"], ddof=1)\n",
        "pop_std_error = std_error * np.sqrt(len(df))\n",
        "print(std_error, pop_std_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683924351025
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "input_attr = attr_mean_bootstrap.groupby(\"input\")[\"attribution\"].mean()\n",
        "plt.plot(np.arange(len(uniques)), input_attr);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "90% confidence interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683924351706
        }
      },
      "outputs": [],
      "source": [
        "point_estimate = np.mean(attr_mean_bootstrap[\"attribution\"])\n",
        "lower = norm.ppf(0.05, loc=point_estimate, scale=std_error)\n",
        "upper = norm.ppf(0.95, loc=point_estimate, scale=std_error)\n",
        "print(lower, upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683924352448
        }
      },
      "outputs": [],
      "source": [
        "attr_mean_bootstrap.to_csv(\"data/block_bootstrap_attr_block_5_deconv_meta.csv\", index=None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Springebenberg et al. 2015- Guided Backpropagation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683924353116
        }
      },
      "outputs": [],
      "source": [
        "guided_backprop = GuidedBackprop(logged_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683924364922
        }
      },
      "outputs": [],
      "source": [
        "gen_attribution_guided = guided_backprop.attribute(inp_data, target=np.argmax(labels, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683924400897
        }
      },
      "outputs": [],
      "source": [
        "plot_frame_attributions(gen_attribution_guided, title=\"Atribuição média do modelo para cada frame do conjunto de validação usando Guided BackPropagation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_audio_attributions(gen_attribution_guided, \"Atribuição média do modelo para cada áudio do conjunto de validação usando Guided BackPropagation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683924437683
        }
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.bar(np.arange(gen_attribution_guided.size()[1]), np.mean(gen_attribution_guided.cpu().detach().numpy(), axis=0))\n",
        "\n",
        "ax.set_xlabel(\"Tempo [s]\")\n",
        "ax.set_ylabel(\"Atribuição\")\n",
        "\n",
        "ax.set_xticks(np.arange(0, gen_attribution_guided.size()[1], int(sr/10)))\n",
        "ax.set_xticklabels(np.arange(0, 0.75, 0.10, dtype=np.float32))\n",
        "\n",
        "ax.set_title(\"Atribuição média do modelo para cada frame do conjunto de validação usando Guided BackPropagation\")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp_df = pd.DataFrame({\"Espécies\": str_labels,\n",
        "    \"Atribuição\": np.mean(gen_attribution_guided.cpu().detach().numpy(), axis=1)})\n",
        "\n",
        "sns.stripplot(x=\"Atribuição\", y=\"Espécies\", data=temp_df, palette=\"deep\", hue=\"Espécies\", legend=False)\n",
        "del temp_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Layer attribution"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### First block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "block_attr_block_1_guided_meta = layer_attribution(\n",
        "    logged_model,\n",
        "    logged_model.base.conv_block1,\n",
        "    \"Conv 1 block\",\n",
        "    inp_data,\n",
        "    (channels, time_steps, mel_bins),\n",
        "    \"guided\",\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683927605596
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(block_attr_block_1_guided_meta)\n",
        "df.to_csv(\"data/block_attr_block_1_guided_meta.csv\", index=None)\n",
        "uniques = df[\"input\"].unique()\n",
        "\n",
        "attr_mean_bootstrap = {\"input\": [], \"attribution\": []}\n",
        "for i in range(10000):\n",
        "    attr_mean_bootstrap[\"attribution\"].extend(rng.choice(df[\"layer_avg_attr\"].values, size=len(uniques)))\n",
        "    \n",
        "for i in range(10000):\n",
        "    attr_mean_bootstrap[\"input\"].extend([j for j in range(len(uniques))])\n",
        "\n",
        "attr_mean_bootstrap = pd.DataFrame(attr_mean_bootstrap)\n",
        "std_error = np.std(attr_mean_bootstrap[\"attribution\"], ddof=1)\n",
        "pop_std_error = std_error * np.sqrt(len(df))\n",
        "print(std_error, pop_std_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683927606456
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "input_attr = attr_mean_bootstrap.groupby(\"input\")[\"attribution\"].mean()\n",
        "plt.plot(np.arange(len(uniques)), input_attr);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "90% confidence interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683927607147
        }
      },
      "outputs": [],
      "source": [
        "point_estimate = np.mean(attr_mean_bootstrap[\"attribution\"])\n",
        "lower = norm.ppf(0.05, loc=point_estimate, scale=std_error)\n",
        "upper = norm.ppf(0.95, loc=point_estimate, scale=std_error)\n",
        "print(lower, upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683927608082
        }
      },
      "outputs": [],
      "source": [
        "attr_mean_bootstrap.to_csv(\"data/block_bootstrap_attr_block_1_guided_meta.csv\", index=None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Second block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "block_attr_block_2_guided_meta = layer_attribution(\n",
        "    logged_model,\n",
        "    logged_model.base.conv_block2,\n",
        "    \"Conv 2 block\",\n",
        "    inp_data,\n",
        "    (channels, time_steps, mel_bins),\n",
        "    \"guided\",\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683931186442
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(block_attr_block_2_guided_meta)\n",
        "df.to_csv(\"data/block_attr_block_2_guided_meta.csv\", index=None)\n",
        "uniques = df[\"input\"].unique()\n",
        "\n",
        "attr_mean_bootstrap = {\"input\": [], \"attribution\": []}\n",
        "for i in range(10000):\n",
        "    attr_mean_bootstrap[\"attribution\"].extend(rng.choice(df[\"layer_avg_attr\"].values, size=len(uniques)))\n",
        "    \n",
        "for i in range(10000):\n",
        "    attr_mean_bootstrap[\"input\"].extend([j for j in range(len(uniques))])\n",
        "\n",
        "attr_mean_bootstrap = pd.DataFrame(attr_mean_bootstrap)\n",
        "std_error = np.std(attr_mean_bootstrap[\"attribution\"], ddof=1)\n",
        "pop_std_error = std_error * np.sqrt(len(df))\n",
        "print(std_error, pop_std_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683931187368
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "input_attr = attr_mean_bootstrap.groupby(\"input\")[\"attribution\"].mean()\n",
        "plt.plot(np.arange(len(uniques)), input_attr);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "90% confidence interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683931188063
        }
      },
      "outputs": [],
      "source": [
        "point_estimate = np.mean(attr_mean_bootstrap[\"attribution\"])\n",
        "lower = norm.ppf(0.05, loc=point_estimate, scale=std_error)\n",
        "upper = norm.ppf(0.95, loc=point_estimate, scale=std_error)\n",
        "print(lower, upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683931188831
        }
      },
      "outputs": [],
      "source": [
        "attr_mean_bootstrap.to_csv(\"data/block_bootstrap_attr_block_2_guided_meta.csv\", index=None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5th block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683936357557
        }
      },
      "outputs": [],
      "source": [
        "time_steps_5th_layer = 2\n",
        "mel_bins_5th_layer = 2\n",
        "channels_5th_layer = 90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "block_attr_block_5_guided_meta = layer_attribution(\n",
        "    logged_model,\n",
        "    logged_model.base.conv_block5,\n",
        "    \"Conv 5 block\",\n",
        "    inp_data,\n",
        "    (channels_5th_layer, time_steps_5th_layer, mel_bins_5th_layer),\n",
        "    \"guided\",\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683940809560
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(block_attr_block_5_guided_meta)\n",
        "df.to_csv(\"data/block_attr_block_5_guided_meta.csv\", index=None)\n",
        "uniques = df[\"input\"].unique()\n",
        "\n",
        "attr_mean_bootstrap = {\"input\": [], \"attribution\": []}\n",
        "for i in range(10000):\n",
        "    attr_mean_bootstrap[\"attribution\"].extend(rng.choice(df[\"layer_avg_attr\"].values, size=len(uniques)))\n",
        "    \n",
        "for i in range(10000):\n",
        "    attr_mean_bootstrap[\"input\"].extend([j for j in range(len(uniques))])\n",
        "\n",
        "attr_mean_bootstrap = pd.DataFrame(attr_mean_bootstrap)\n",
        "std_error = np.std(attr_mean_bootstrap[\"attribution\"], ddof=1)\n",
        "pop_std_error = std_error * np.sqrt(len(df))\n",
        "print(std_error, pop_std_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683940810540
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "input_attr = attr_mean_bootstrap.groupby(\"input\")[\"attribution\"].mean()\n",
        "plt.plot(np.arange(len(uniques)), input_attr);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "90% confidence interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683940868363
        }
      },
      "outputs": [],
      "source": [
        "point_estimate = np.mean(attr_mean_bootstrap[\"attribution\"])\n",
        "lower = norm.ppf(0.05, loc=point_estimate, scale=std_error)\n",
        "upper = norm.ppf(0.95, loc=point_estimate, scale=std_error)\n",
        "print(lower, upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683940872951
        }
      },
      "outputs": [],
      "source": [
        "attr_mean_bootstrap.to_csv(\"data/block_bootstrap_attr_block_5_guided_meta.csv\", index=None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "deconv_1st_block = pd.read_csv(\"data/block_bootstrap_attr_block_1_deconv_meta.csv\")\n",
        "deconv_2nd_block = pd.read_csv(\"data/block_bootstrap_attr_block_2_deconv_meta.csv\")\n",
        "deconv_5th_block = pd.read_csv(\"data/block_bootstrap_attr_block_5_deconv_meta.csv\")\n",
        "\n",
        "guided_1st_block = pd.read_csv(\"data/block_bootstrap_attr_block_1_guided_meta.csv\")\n",
        "guided_2nd_block = pd.read_csv(\"data/block_bootstrap_attr_block_2_guided_meta.csv\")\n",
        "guided_5th_block = pd.read_csv(\"data/block_bootstrap_attr_block_5_guided_meta.csv\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1st block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_, axis = plt.subplots()\n",
        "\n",
        "grouped_1st_deconv_block = deconv_1st_block.groupby(\"input\")\n",
        "\n",
        "sns.stripplot(x=grouped_1st_deconv_block[\"attribution\"].mean(), y=str_labels,\n",
        "        palette=\"deep\", hue=str_labels, ax=axis, legend=False)\n",
        "\n",
        "axis.set_xlabel(\"Atribuição\")\n",
        "axis.set_ylabel(\"Espécies\")\n",
        "plt.show()\n",
        "\n",
        "_, axis = plt.subplots()\n",
        "\n",
        "grouped_1st_guided_block = guided_1st_block.groupby(\"input\")\n",
        "\n",
        "sns.stripplot(x=grouped_1st_guided_block[\"attribution\"].mean(), y=str_labels,\n",
        "        palette=\"deep\", hue=str_labels, ax=axis, legend=False)\n",
        "\n",
        "axis.set_xlabel(\"Atribuição\")\n",
        "axis.set_ylabel(\"Espécies\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2nd block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_, axis = plt.subplots()\n",
        "\n",
        "grouped_2nd_deconv_block = deconv_2nd_block.groupby(\"input\")\n",
        "\n",
        "sns.stripplot(x=grouped_2nd_deconv_block[\"attribution\"].mean(), y=str_labels,\n",
        "        palette=\"deep\", hue=str_labels, ax=axis, legend=False)\n",
        "\n",
        "axis.set_xlabel(\"Atribuição\")\n",
        "axis.set_ylabel(\"Espécies\")\n",
        "plt.show()\n",
        "\n",
        "_, axis = plt.subplots()\n",
        "\n",
        "grouped_2nd_guided_block = guided_2nd_block.groupby(\"input\")\n",
        "\n",
        "sns.stripplot(x=grouped_2nd_guided_block[\"attribution\"].mean(), y=str_labels,\n",
        "        palette=\"deep\", hue=str_labels, ax=axis, legend=False)\n",
        "\n",
        "axis.set_xlabel(\"Atribuição\")\n",
        "axis.set_ylabel(\"Espécies\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5th block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_, axis = plt.subplots()\n",
        "\n",
        "grouped_5th_deconv_block = deconv_5th_block.groupby(\"input\")\n",
        "\n",
        "sns.stripplot(x=grouped_5th_deconv_block[\"attribution\"].mean(), y=str_labels,\n",
        "        palette=\"deep\", hue=str_labels, ax=axis, legend=False)\n",
        "\n",
        "axis.set_xlabel(\"Atribuição\")\n",
        "axis.set_ylabel(\"Espécies\")\n",
        "plt.show()\n",
        "\n",
        "_, axis = plt.subplots()\n",
        "\n",
        "grouped_5th_guided_block = guided_5th_block.groupby(\"input\")\n",
        "\n",
        "sns.stripplot(x=grouped_5th_guided_block[\"attribution\"].mean(), y=str_labels,\n",
        "        palette=\"deep\", hue=str_labels, ax=axis, legend=False)\n",
        "\n",
        "axis.set_xlabel(\"Atribuição\")\n",
        "axis.set_ylabel(\"Espécies\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Largest attributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "largest_attr_1st_block_deconv = deconv_1st_block.groupby(\"input\")[\"attribution\"]\\\n",
        "    .sum().nlargest(3)\n",
        "\n",
        "largest_attr_2nd_block_deconv = deconv_2nd_block.groupby(\"input\")[\"attribution\"]\\\n",
        "    .sum().nlargest(3)\n",
        "\n",
        "# largest mean scale (1e-8)\n",
        "largest_attr_5th_block_deconv = deconv_5th_block.groupby(\"input\")[\"attribution\"]\\\n",
        "    .sum().nlargest(4)\n",
        "\n",
        "print(largest_attr_1st_block_deconv, largest_attr_2nd_block_deconv, largest_attr_5th_block_deconv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "largest_attr_1st_block_guided = guided_1st_block.groupby(\"input\")[\"attribution\"]\\\n",
        "    .sum().nlargest(3)\n",
        "\n",
        "largest_attr_2nd_block_guided = guided_2nd_block.groupby(\"input\")[\"attribution\"]\\\n",
        "    .sum().nlargest(3)\n",
        "\n",
        "# greatest number of impactul attributions\n",
        "largest_attr_5th_block_guided = guided_5th_block.groupby(\"input\")[\"attribution\"]\\\n",
        "    .sum().nlargest(4)\n",
        "\n",
        "print(largest_attr_1st_block_guided, largest_attr_2nd_block_guided, largest_attr_5th_block_guided)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_inputs = largest_attr_1st_block_deconv.index.union(\n",
        "    largest_attr_2nd_block_deconv.index\n",
        "    ).union(\n",
        "        largest_attr_5th_block_deconv.index\n",
        "        ).union(\n",
        "            largest_attr_1st_block_guided.index\n",
        "        ).union(\n",
        "            largest_attr_2nd_block_guided.index\n",
        "        ).union(\n",
        "            largest_attr_5th_block_guided.index\n",
        "        )\n",
        "\n",
        "unique_inputs, len(unique_inputs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Iterate over each audio considering the parameters for spectrogram generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683934398051
        }
      },
      "outputs": [],
      "source": [
        "start = 0\n",
        "num_frame = 1\n",
        "num_audios = len(inp_data)\n",
        "\n",
        "while cur_window + window_size - 1 < inp_data.shape[1]:\n",
        "\n",
        "    for i in range(num_audios):\n",
        "        frame = inp_data[i][cur_window:cur_window+window_size]\n",
        "\n",
        "        frame_zcr = zero_crossing_rate(frame.cpu().detach().numpy())\n",
        "        zcr_audios[\"avgZcr\"].append(np.mean(frame_zcr))\n",
        "        zcr_audios[\"label\"].append(idx_to_label[np.argmax(labels.cpu().detach().numpy())])\n",
        "        zcr_audios[\"Período (ms)\"].append(num_frame)\n",
        "\n",
        "    num_frame += 1\n",
        "    cur_window += hop_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683934398121
        }
      },
      "outputs": [],
      "source": [
        "zcr_audios = pd.DataFrame(zcr_audios)\n",
        "zcr_audios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683934398181
        }
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots()\n",
        "sns.histplot(x=\"avgZcr\", data=zcr_audios, element=\"step\", fill=False, ax=ax);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683934398247
        }
      },
      "outputs": [],
      "source": [
        "first_18_frames_zcr_audios = zcr_audios[zcr_audios[\"Período (ms)\"] <= 18]\n",
        "frames_18_36_zcr_audios = zcr_audios[(zcr_audios[\"Período (ms)\"] > 18) & (zcr_audios[\"Período (ms)\"] <= 36)]\n",
        "frames_36_54_zcr_audios = zcr_audios[(zcr_audios[\"Período (ms)\"] > 36) & (zcr_audios[\"Período (ms)\"] <= 54)]\n",
        "frames_54_above_zcr_audios = zcr_audios[zcr_audios[\"Período (ms)\"] > 54]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683934398313
        }
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots()\n",
        "sns.histplot(x=\"avgZcr\", data=first_18_frames_zcr_audios, hue=\"Período (ms)\", element=\"step\", fill=False, ax=ax);\n",
        "ax.set_xlabel(\"Taxa média de cruzamento de zero\")\n",
        "ax.set_ylabel(\"Quantidade\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683934398392
        }
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots()\n",
        "sns.histplot(x=\"avgZcr\", data=frames_18_36_zcr_audios, hue=\"Período (ms)\", element=\"step\", fill=False, ax=ax);\n",
        "ax.set_xlabel(\"Taxa média de cruzamento de zero\")\n",
        "ax.set_ylabel(\"Quantidade\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683934398604
        }
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots()\n",
        "sns.histplot(x=\"avgZcr\", data=frames_36_54_zcr_audios, hue=\"Período (ms)\", element=\"step\", fill=False, ax=ax);\n",
        "ax.set_xlabel(\"Taxa média de cruzamento de zero\")\n",
        "ax.set_ylabel(\"Quantidade\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683934398674
        }
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots()\n",
        "sns.histplot(x=\"avgZcr\", data=frames_54_above_zcr_audios, hue=\"Período (ms)\", element=\"step\", fill=False, ax=ax);\n",
        "ax.set_xlabel(\"Taxa média de cruzamento de zero\")\n",
        "ax.set_ylabel(\"Quantidade\");"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check spectrograms with largest attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683934398739
        }
      },
      "outputs": [],
      "source": [
        "axes = []\n",
        "\n",
        "for i, audio_ind in enumerate(unique_inputs):\n",
        "    _, axis = plt.subplots(figsize=(6, 3))\n",
        "    axes.append(axis)\n",
        "\n",
        "    librosa.display.specshow(np.abs(\n",
        "        librosa.stft(inp_data[audio_ind].cpu().detach().numpy(),\n",
        "            n_fft=window_size, win_length=window_size, hop_length=hop_size, center=True)\n",
        "        ),\n",
        "        sr=sr, x_axis=\"time\", y_axis=\"linear\", hop_length=hop_size,\n",
        "        fmin=int(tags[\"fmin\"]), fmax=int(tags[\"fmax\"]), ax=axes[i]\n",
        "    )\n",
        "    label = idx_to_label[np.argmax(labels[audio_ind].cpu().detach().numpy())]\n",
        "\n",
        "    axes[i].set_title(f\"Espectrograma- {'OtherBirds' if label == 'others' else label}\",\n",
        "                    {'fontsize': 11})\n",
        "\n",
        "    axes[i].set_xlabel(\"Tempo [s]\")\n",
        "    \n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "panns-EXV_f7Qi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
